{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bootstrapping_Classification_S5.5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"kaFm1XuOoWzM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":890},"outputId":"5584b635-0732-4b95-95a0-af676b103470","executionInfo":{"status":"ok","timestamp":1576511930306,"user_tz":-210,"elapsed":132201,"user":{"displayName":"MohammadMahdi Moradi","photoUrl":"","userId":"10128289049495336752"}}},"source":["%tensorflow_version 2.x\n","import os\n","import time\n","import statistics\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","from scipy.stats import zscore\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.models import Sequential\n","from sklearn.model_selection import ShuffleSplit\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Dense, Activation\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","\n","## Read the data set\n","df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\", na_values=['NA','?'])\n","\n","\n","## Generate dummies for job\n","df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n","df.drop('job', axis=1, inplace=True)\n","\n","\n","## Generate dummies for area\n","df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n","df.drop('area', axis=1, inplace=True)\n","\n","\n","## Missing values for income\n","df['income'] = df['income'].fillna(df['income'].median())\n","\n","\n","## Standardize ranges\n","df['income'] = zscore(df['income'])\n","df['aspect'] = zscore(df['aspect'])\n","df['save_rate'] = zscore(df['save_rate'])\n","df['age'] = zscore(df['age'])\n","df['subscriptions'] = zscore(df['subscriptions'])\n","\n","\n","## Convert to numpy - Classification\n","x_columns = df.columns.drop('product').drop('id')\n","x = df[x_columns].values\n","dummies = pd.get_dummies(df['product']) # Classification\n","products = dummies.columns\n","y = dummies.values\n","\n","\n","## BootStrap\n","boot = StratifiedShuffleSplit(n_splits=50, test_size=0.1, random_state=42)\n","\n","num = 0\n","epochs_needed = []\n","Log_Losses = []\n","print(\"number    score    mean score    stdev    epochs    mean epochs    \")\n","print(\"------   -------   ----------   -------   ------    -----------\")\n","\n","for train, test in boot.split(x, df['product']):\n","  num += 1\n","\n","  # Spliting to training and testing set\n","  x_train = x[train]\n","  y_train = y[train]\n","  x_test = x[test]\n","  y_test = y[test]\n","\n","  # Construct neural network\n","  model = Sequential()\n","  model.add(Dense(50, input_dim=x.shape[1], activation='relu'))          # Hidden 1\n","  model.add(Dense(25, activation='relu'))           # Hidden 2\n","  model.add(Dense(y.shape[1],activation='softmax'))           # Output\n","  model.compile(loss='categorical_crossentropy', optimizer='adam')\n","  monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=25, verbose=0, mode='auto', restore_best_weights=True)\n","\n","  # Train on the bootstrap sample\n","  model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n","  epochs = monitor.stopped_epoch\n","  epochs_needed.append(epochs)\n","\n","  # Predict on the validation set\n","  pred = model.predict(x_test)\n","  ##################### y_hat = np.argmax(pred, axis=1)\n","  y_true = np.argmax(y_test, axis=1)\n","  #####################print(pred)\n","  #####################print(y_hat)\n","  ##################### print(y_true)\n","  # Calculatng Log Loss\n","  score = metrics.log_loss(y_true, pred)\n","  Log_Losses.append(score)\n","\n","  # Mean and standard deviation calculation\n","  epochs_mean = statistics.mean(epochs_needed)\n","  scores_mean = statistics.mean(Log_Losses)\n","  scores_std = statistics.pstdev(Log_Losses)\n","\n","  # Recording Section\n","  print(f\" #{num}      {score:.4f}      {scores_mean:.4f}     {scores_std:.4f}      {epochs}          {int(epochs_mean)}\")\n","\n"],"execution_count":42,"outputs":[{"output_type":"stream","text":["number    score    mean score    stdev    epochs    mean epochs    \n","------   -------   ----------   -------   ------    -----------\n"," #1      0.6771      0.6771     0.0000      62          62\n"," #2      0.6678      0.6724     0.0046      49          55\n"," #3      0.6822      0.6757     0.0059      42          51\n"," #4      0.6807      0.6769     0.0056      90          60\n"," #5      0.6837      0.6783     0.0057      61          60\n"," #6      0.7004      0.6820     0.0097      55          59\n"," #7      0.7083      0.6857     0.0129      51          58\n"," #8      0.7581      0.6948     0.0268      43          56\n"," #9      0.6197      0.6864     0.0346      71          58\n"," #10      0.6673      0.6845     0.0333      61          58\n"," #11      0.6974      0.6857     0.0319      54          58\n"," #12      0.7345      0.6898     0.0334      53          57\n"," #13      0.6964      0.6903     0.0322      78          59\n"," #14      0.6855      0.6899     0.0310      76          60\n"," #15      0.6605      0.6880     0.0309      60          60\n"," #16      0.7232      0.6902     0.0311      49          59\n"," #17      0.6718      0.6891     0.0304      74          60\n"," #18      0.6289      0.6857     0.0326      75          61\n"," #19      0.6146      0.6820     0.0355      50          60\n"," #20      0.6902      0.6824     0.0347      63          60\n"," #21      0.6402      0.6804     0.0350      59          60\n"," #22      0.7500      0.6836     0.0371      47          60\n"," #23      0.5744      0.6788     0.0426      104          62\n"," #24      0.7180      0.6804     0.0424      75          62\n"," #25      0.5447      0.6750     0.0494      81          63\n"," #26      0.6844      0.6754     0.0484      59          63\n"," #27      0.7084      0.6766     0.0479      73          63\n"," #28      0.7166      0.6780     0.0477      62          63\n"," #29      0.6718      0.6778     0.0468      66          63\n"," #30      0.7347      0.6797     0.0472      53          63\n"," #31      0.7267      0.6812     0.0471      47          62\n"," #32      0.6733      0.6810     0.0464      80          63\n"," #33      0.6425      0.6798     0.0462      61          63\n"," #34      0.5898      0.6772     0.0480      76          63\n"," #35      0.5007      0.6721     0.0557      100          64\n"," #36      0.6419      0.6713     0.0551      63          64\n"," #37      0.6821      0.6716     0.0544      57          64\n"," #38      0.7090      0.6726     0.0540      59          64\n"," #39      0.7631      0.6749     0.0552      57          64\n"," #40      0.7066      0.6757     0.0547      40          63\n"," #41      0.7217      0.6768     0.0545      45          62\n"," #42      0.6519      0.6762     0.0540      53          62\n"," #43      0.6980      0.6767     0.0535      54          62\n"," #44      0.7491      0.6784     0.0540      69          62\n"," #45      0.6518      0.6778     0.0535      64          62\n"," #46      0.6399      0.6769     0.0532      76          62\n"," #47      0.6768      0.6769     0.0526      55          62\n"," #48      0.7304      0.6781     0.0526      70          62\n"," #49      0.6425      0.6773     0.0523      96          63\n"," #50      0.6913      0.6776     0.0518      53          63\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6lyH_YAqU2L4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}