{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classification_EarlyStopping.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"FJnJZ3TSj7XS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"66fb79e9-f347-4cea-8d04-b8fc5e37fb94","executionInfo":{"status":"ok","timestamp":1576084457975,"user_tz":-210,"elapsed":5565,"user":{"displayName":"MohammadMahdi Moradi","photoUrl":"","userId":"10128289049495336752"}}},"source":["%tensorflow_version 2.x\n","import os\n","import requests\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.metrics import accuracy_score\n","\n","## Getting data from source\n","df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/iris.csv\", na_values=['NA','?'])\n","headers = df.columns.values\n","print(headers)\n","## Specifying the unique value of output\n","labels = list(df['species'].unique())\n","\n","## producing dummy for output which is name of flower\n","dummies = pd.get_dummies(df['species'], prefix='species')\n","df = pd.concat([df, dummies], axis=1)\n","df.drop('species', axis=1, inplace=True)\n","\n","## making  training data\n","x = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n","y = dummies.values\n","\n","#= Split training data into validation and training sets\n","x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size=0.25, random_state=42)\n","\n","## Building Nueral Network\n","model = Sequential()\n","model.add(Dense(50, input_dim=x.shape[1], activation='relu'))\n","model.add(Dense(25, activation='relu'))\n","model.add(Dense(y.shape[1], activation= 'softmax'))\n","\n","## Training Process and EARLYSTOPPING PROCESS\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto', restore_best_weights=True)\n","model.fit(x_train, y_train, validation_data=(x_validation, y_validation), callbacks=[monitor], verbose=1, epochs=1000)\n","\n","## Calculating accuracy\n","pred = model.predict(x_validation)\n","predict_class = np.argmax(pred, axis=1)         # it gives an list which is index of the maximum for each predicted\n","expected_class = np.argmax(y_validation, axis=1)          # it gives an list which is index of the maximum for each output\n","correct = accuracy_score(predict_class, expected_class)\n","print(f\"accuracy = {correct}\")\n","\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["['sepal_l' 'sepal_w' 'petal_l' 'petal_w' 'species']\n","Train on 112 samples, validate on 38 samples\n","Epoch 1/1000\n","112/112 [==============================] - 0s 4ms/sample - loss: 1.1489 - val_loss: 1.0323\n","Epoch 2/1000\n","112/112 [==============================] - 0s 282us/sample - loss: 1.0093 - val_loss: 0.9768\n","Epoch 3/1000\n","112/112 [==============================] - 0s 291us/sample - loss: 0.9844 - val_loss: 0.9695\n","Epoch 4/1000\n","112/112 [==============================] - 0s 249us/sample - loss: 0.9689 - val_loss: 0.9462\n","Epoch 5/1000\n","112/112 [==============================] - 0s 276us/sample - loss: 0.9436 - val_loss: 0.9102\n","Epoch 6/1000\n","112/112 [==============================] - 0s 250us/sample - loss: 0.9098 - val_loss: 0.8713\n","Epoch 7/1000\n","112/112 [==============================] - 0s 244us/sample - loss: 0.8787 - val_loss: 0.8409\n","Epoch 8/1000\n","112/112 [==============================] - 0s 233us/sample - loss: 0.8559 - val_loss: 0.8135\n","Epoch 9/1000\n","112/112 [==============================] - 0s 253us/sample - loss: 0.8292 - val_loss: 0.7841\n","Epoch 10/1000\n","112/112 [==============================] - 0s 248us/sample - loss: 0.7964 - val_loss: 0.7520\n","Epoch 11/1000\n","112/112 [==============================] - 0s 260us/sample - loss: 0.7674 - val_loss: 0.7187\n","Epoch 12/1000\n","112/112 [==============================] - 0s 239us/sample - loss: 0.7385 - val_loss: 0.6869\n","Epoch 13/1000\n","112/112 [==============================] - 0s 260us/sample - loss: 0.7111 - val_loss: 0.6578\n","Epoch 14/1000\n","112/112 [==============================] - 0s 277us/sample - loss: 0.6796 - val_loss: 0.6232\n","Epoch 15/1000\n","112/112 [==============================] - 0s 269us/sample - loss: 0.6515 - val_loss: 0.5912\n","Epoch 16/1000\n","112/112 [==============================] - 0s 281us/sample - loss: 0.6223 - val_loss: 0.5662\n","Epoch 17/1000\n","112/112 [==============================] - 0s 255us/sample - loss: 0.6012 - val_loss: 0.5425\n","Epoch 18/1000\n","112/112 [==============================] - 0s 260us/sample - loss: 0.5768 - val_loss: 0.5204\n","Epoch 19/1000\n","112/112 [==============================] - 0s 240us/sample - loss: 0.5553 - val_loss: 0.4979\n","Epoch 20/1000\n","112/112 [==============================] - 0s 235us/sample - loss: 0.5347 - val_loss: 0.4747\n","Epoch 21/1000\n","112/112 [==============================] - 0s 238us/sample - loss: 0.5098 - val_loss: 0.4441\n","Epoch 22/1000\n","112/112 [==============================] - 0s 261us/sample - loss: 0.4824 - val_loss: 0.4263\n","Epoch 23/1000\n","112/112 [==============================] - 0s 246us/sample - loss: 0.4621 - val_loss: 0.4081\n","Epoch 24/1000\n","112/112 [==============================] - 0s 237us/sample - loss: 0.4443 - val_loss: 0.3889\n","Epoch 25/1000\n","112/112 [==============================] - 0s 231us/sample - loss: 0.4271 - val_loss: 0.3712\n","Epoch 26/1000\n","112/112 [==============================] - 0s 228us/sample - loss: 0.4117 - val_loss: 0.3553\n","Epoch 27/1000\n","112/112 [==============================] - 0s 281us/sample - loss: 0.3958 - val_loss: 0.3425\n","Epoch 28/1000\n","112/112 [==============================] - 0s 255us/sample - loss: 0.3804 - val_loss: 0.3286\n","Epoch 29/1000\n","112/112 [==============================] - 0s 240us/sample - loss: 0.3657 - val_loss: 0.3155\n","Epoch 30/1000\n","112/112 [==============================] - 0s 315us/sample - loss: 0.3579 - val_loss: 0.3032\n","Epoch 31/1000\n","112/112 [==============================] - 0s 251us/sample - loss: 0.3418 - val_loss: 0.2929\n","Epoch 32/1000\n","112/112 [==============================] - 0s 251us/sample - loss: 0.3351 - val_loss: 0.2935\n","Epoch 33/1000\n","112/112 [==============================] - 0s 277us/sample - loss: 0.3243 - val_loss: 0.2768\n","Epoch 34/1000\n","112/112 [==============================] - 0s 254us/sample - loss: 0.3089 - val_loss: 0.2648\n","Epoch 35/1000\n","112/112 [==============================] - 0s 262us/sample - loss: 0.2995 - val_loss: 0.2554\n","Epoch 36/1000\n","112/112 [==============================] - 0s 277us/sample - loss: 0.2911 - val_loss: 0.2462\n","Epoch 37/1000\n","112/112 [==============================] - 0s 241us/sample - loss: 0.2809 - val_loss: 0.2390\n","Epoch 38/1000\n","112/112 [==============================] - 0s 260us/sample - loss: 0.2739 - val_loss: 0.2354\n","Epoch 39/1000\n","112/112 [==============================] - 0s 219us/sample - loss: 0.2647 - val_loss: 0.2236\n","Epoch 40/1000\n","112/112 [==============================] - 0s 243us/sample - loss: 0.2536 - val_loss: 0.2136\n","Epoch 41/1000\n","112/112 [==============================] - 0s 241us/sample - loss: 0.2460 - val_loss: 0.2063\n","Epoch 42/1000\n","112/112 [==============================] - 0s 230us/sample - loss: 0.2374 - val_loss: 0.2022\n","Epoch 43/1000\n","112/112 [==============================] - 0s 267us/sample - loss: 0.2310 - val_loss: 0.1976\n","Epoch 44/1000\n","112/112 [==============================] - 0s 226us/sample - loss: 0.2252 - val_loss: 0.1864\n","Epoch 45/1000\n","112/112 [==============================] - 0s 240us/sample - loss: 0.2174 - val_loss: 0.1809\n","Epoch 46/1000\n","112/112 [==============================] - 0s 214us/sample - loss: 0.2108 - val_loss: 0.1775\n","Epoch 47/1000\n","112/112 [==============================] - 0s 228us/sample - loss: 0.2054 - val_loss: 0.1744\n","Epoch 48/1000\n","112/112 [==============================] - 0s 225us/sample - loss: 0.2019 - val_loss: 0.1668\n","Epoch 49/1000\n","112/112 [==============================] - 0s 277us/sample - loss: 0.1952 - val_loss: 0.1637\n","Epoch 50/1000\n","112/112 [==============================] - 0s 235us/sample - loss: 0.1900 - val_loss: 0.1651\n","Epoch 51/1000\n","112/112 [==============================] - 0s 236us/sample - loss: 0.1883 - val_loss: 0.1585\n","Epoch 52/1000\n","112/112 [==============================] - 0s 234us/sample - loss: 0.1812 - val_loss: 0.1496\n","Epoch 53/1000\n","112/112 [==============================] - 0s 234us/sample - loss: 0.1798 - val_loss: 0.1459\n","Epoch 54/1000\n","112/112 [==============================] - 0s 245us/sample - loss: 0.1751 - val_loss: 0.1443\n","Epoch 55/1000\n","112/112 [==============================] - 0s 216us/sample - loss: 0.1729 - val_loss: 0.1548\n","Epoch 56/1000\n","112/112 [==============================] - 0s 251us/sample - loss: 0.1735 - val_loss: 0.1400\n","Epoch 57/1000\n","112/112 [==============================] - 0s 276us/sample - loss: 0.1637 - val_loss: 0.1356\n","Epoch 58/1000\n","112/112 [==============================] - 0s 247us/sample - loss: 0.1624 - val_loss: 0.1326\n","Epoch 59/1000\n","112/112 [==============================] - 0s 229us/sample - loss: 0.1599 - val_loss: 0.1360\n","Epoch 60/1000\n","112/112 [==============================] - 0s 245us/sample - loss: 0.1576 - val_loss: 0.1310\n","Epoch 61/1000\n","112/112 [==============================] - 0s 246us/sample - loss: 0.1515 - val_loss: 0.1229\n","Epoch 62/1000\n","112/112 [==============================] - 0s 268us/sample - loss: 0.1529 - val_loss: 0.1203\n","Epoch 63/1000\n","112/112 [==============================] - 0s 262us/sample - loss: 0.1512 - val_loss: 0.1239\n","Epoch 64/1000\n","112/112 [==============================] - 0s 244us/sample - loss: 0.1467 - val_loss: 0.1230\n","Epoch 65/1000\n","112/112 [==============================] - 0s 231us/sample - loss: 0.1428 - val_loss: 0.1163\n","Epoch 66/1000\n","112/112 [==============================] - 0s 237us/sample - loss: 0.1425 - val_loss: 0.1118\n","Epoch 67/1000\n","112/112 [==============================] - 0s 224us/sample - loss: 0.1433 - val_loss: 0.1127\n","Epoch 68/1000\n","112/112 [==============================] - 0s 249us/sample - loss: 0.1369 - val_loss: 0.1138\n","Epoch 69/1000\n","112/112 [==============================] - 0s 250us/sample - loss: 0.1369 - val_loss: 0.1209\n","Epoch 70/1000\n","112/112 [==============================] - 0s 269us/sample - loss: 0.1364 - val_loss: 0.1126\n","Epoch 71/1000\n","112/112 [==============================] - 0s 246us/sample - loss: 0.1330 - val_loss: 0.1043\n","Epoch 72/1000\n","112/112 [==============================] - 0s 297us/sample - loss: 0.1310 - val_loss: 0.1041\n","Epoch 73/1000\n","112/112 [==============================] - 0s 236us/sample - loss: 0.1322 - val_loss: 0.1126\n","Epoch 74/1000\n","112/112 [==============================] - 0s 222us/sample - loss: 0.1304 - val_loss: 0.1034\n","Epoch 75/1000\n","112/112 [==============================] - 0s 229us/sample - loss: 0.1274 - val_loss: 0.1031\n","Epoch 76/1000\n","112/112 [==============================] - 0s 229us/sample - loss: 0.1255 - val_loss: 0.0979\n","Epoch 77/1000\n","112/112 [==============================] - 0s 253us/sample - loss: 0.1230 - val_loss: 0.1019\n","Epoch 78/1000\n","112/112 [==============================] - 0s 220us/sample - loss: 0.1233 - val_loss: 0.0986\n","Epoch 79/1000\n","112/112 [==============================] - 0s 236us/sample - loss: 0.1203 - val_loss: 0.0972\n","Epoch 80/1000\n","112/112 [==============================] - 0s 245us/sample - loss: 0.1214 - val_loss: 0.0922\n","Epoch 81/1000\n","112/112 [==============================] - 0s 247us/sample - loss: 0.1182 - val_loss: 0.0931\n","Epoch 82/1000\n","112/112 [==============================] - 0s 233us/sample - loss: 0.1166 - val_loss: 0.0953\n","Epoch 83/1000\n","112/112 [==============================] - 0s 278us/sample - loss: 0.1194 - val_loss: 0.1015\n","Epoch 84/1000\n","112/112 [==============================] - 0s 241us/sample - loss: 0.1176 - val_loss: 0.0881\n","Epoch 85/1000\n","112/112 [==============================] - 0s 239us/sample - loss: 0.1141 - val_loss: 0.0877\n","Epoch 86/1000\n","112/112 [==============================] - 0s 211us/sample - loss: 0.1131 - val_loss: 0.0898\n","Epoch 87/1000\n","112/112 [==============================] - 0s 253us/sample - loss: 0.1121 - val_loss: 0.0965\n","Epoch 88/1000\n","112/112 [==============================] - 0s 225us/sample - loss: 0.1130 - val_loss: 0.0922\n","Epoch 89/1000\n","112/112 [==============================] - 0s 273us/sample - loss: 0.1125 - val_loss: 0.0820\n","Epoch 90/1000\n","112/112 [==============================] - 0s 286us/sample - loss: 0.1118 - val_loss: 0.0828\n","Epoch 91/1000\n","112/112 [==============================] - 0s 255us/sample - loss: 0.1104 - val_loss: 0.0864\n","Epoch 92/1000\n","112/112 [==============================] - 0s 254us/sample - loss: 0.1107 - val_loss: 0.0797\n","Epoch 93/1000\n","112/112 [==============================] - 0s 265us/sample - loss: 0.1096 - val_loss: 0.0829\n","Epoch 94/1000\n","112/112 [==============================] - 0s 248us/sample - loss: 0.1083 - val_loss: 0.0785\n","Epoch 95/1000\n","112/112 [==============================] - 0s 255us/sample - loss: 0.1067 - val_loss: 0.0852\n","Epoch 96/1000\n","112/112 [==============================] - 0s 257us/sample - loss: 0.1043 - val_loss: 0.0832\n","Epoch 97/1000\n","112/112 [==============================] - 0s 248us/sample - loss: 0.1044 - val_loss: 0.0824\n","Epoch 98/1000\n","112/112 [==============================] - 0s 253us/sample - loss: 0.1029 - val_loss: 0.0772\n","Epoch 99/1000\n","112/112 [==============================] - 0s 249us/sample - loss: 0.1019 - val_loss: 0.0756\n","Epoch 100/1000\n","112/112 [==============================] - 0s 297us/sample - loss: 0.1016 - val_loss: 0.0758\n","Epoch 101/1000\n","112/112 [==============================] - 0s 298us/sample - loss: 0.1010 - val_loss: 0.0796\n","Epoch 102/1000\n","112/112 [==============================] - 0s 238us/sample - loss: 0.1057 - val_loss: 0.0947\n","Epoch 103/1000\n","112/112 [==============================] - 0s 236us/sample - loss: 0.1015 - val_loss: 0.0761\n","Epoch 104/1000\n","112/112 [==============================] - 0s 244us/sample - loss: 0.0978 - val_loss: 0.0686\n","Epoch 105/1000\n","112/112 [==============================] - 0s 236us/sample - loss: 0.1096 - val_loss: 0.0680\n","Epoch 106/1000\n","112/112 [==============================] - 0s 241us/sample - loss: 0.1011 - val_loss: 0.0976\n","Epoch 107/1000\n","112/112 [==============================] - 0s 256us/sample - loss: 0.1145 - val_loss: 0.1049\n","Epoch 108/1000\n","112/112 [==============================] - 0s 252us/sample - loss: 0.1036 - val_loss: 0.0679\n","Epoch 109/1000\n","112/112 [==============================] - 0s 293us/sample - loss: 0.0985 - val_loss: 0.0657\n","Epoch 110/1000\n","112/112 [==============================] - 0s 243us/sample - loss: 0.1008 - val_loss: 0.0665\n","Epoch 111/1000\n","112/112 [==============================] - 0s 229us/sample - loss: 0.0977 - val_loss: 0.0717\n","Epoch 112/1000\n","112/112 [==============================] - 0s 250us/sample - loss: 0.0950 - val_loss: 0.0723\n","Epoch 113/1000\n","112/112 [==============================] - 0s 240us/sample - loss: 0.0937 - val_loss: 0.0668\n","Epoch 114/1000\n"," 32/112 [=======>......................] - ETA: 0s - loss: 0.0954Restoring model weights from the end of the best epoch.\n","112/112 [==============================] - 0s 272us/sample - loss: 0.0966 - val_loss: 0.0649\n","Epoch 00114: early stopping\n","accuracy = 1.0\n"],"name":"stdout"}]}]}